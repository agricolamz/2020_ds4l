---
title: "Коэффициент Байеса"
author: "G. Moroz"
date: "10/03/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(latex2exp)
library(tidyverse)
theme_set(theme_bw())
```

```{r}
library(tidyverse)
```

## 1. Вероятность vs. функция правдободобия

Предположим что распределение количества согласных в языках мира можно описать логнормальным распределением с логсредним 3, и логстандартным отклонением 0.37:

```{r, echo=FALSE}
tibble(x = 0:80) %>% 
  ggplot(aes(x)) +
  stat_function(fun = function(x) dlnorm(x, 3, 0.37))+
  labs(title = "Количество согласных в языках мира",
       caption = "по данным PHOIBLE",
       x = "количество согласных",
       y = "")
```

Тогда вероятность того, что в выбранных произвольно языках окажется от 23 до 32 согласных равна интегралу нормального распределения в указанном промежутке:

```{r, echo = FALSE}
tibble(x = 0:80) %>% 
  ggplot(aes(x)) +
  stat_function(fun = function(x) dlnorm(x, 3, 0.37))+
  stat_function(fun = function(x) dlnorm(x, 3, 0.37), 
                xlim = c(23,32), 
                geom = "area",
                fill = "lightblue")+
  labs(title = "Количество согласных в языках мира",
       caption = "по данным PHOIBLE",
       x = "количество согласных",
       y = "")
```

$$P\left(X \in (23,\, 32) | X \sim \ln\mathcal{N}(\mu = 3,\, \sigma^{2}=0.37)\right) = ...$$

```{r}
plnorm(32, meanlog = 3, sdlog = 0.37) - plnorm(23, meanlog = 3, sdlog = 0.37)
```

Когда мы говорим про функцию правдоподобия, то мы уже нашли еще один язык в котором оказалось 33 согласных. Нас интересует, насколько правдоподобна функция логнормального распределения с логсредним 3 и логстандартным отклонением 0.37 при значении переменной 33. Это значение равно функции плотности:

```{r, echo = FALSE}
tibble(x = 0:80) %>% 
  ggplot(aes(x)) +
  stat_function(fun = function(x) dlnorm(x, 3, 0.37))+
  geom_segment(aes(x = 33, xend = 33, y = 0, yend = dlnorm(33, 3, 0.37)), color = "red")+
  geom_segment(aes(x = 33, xend = 0, y = dlnorm(33, 3, 0.37), yend = dlnorm(33, 3, 0.37)), color = "red",
               arrow = arrow(length = unit(0.03, "npc")))+
  scale_x_continuous(breaks = c(0:4*20, 33))+
  scale_y_continuous(breaks = c(0:3*0.02, round(dlnorm(33, 3, 0.37), 3)))+
  labs(title = "Количество согласных в языках мира",
       caption = "по данным PHOIBLE",
       x = "количество согласных",
       y = "")
```

$$L\left(X \sim \ln\mathcal{N}(\mu = 3,\, \sigma^{2}=0.37)|x = 33\right) = ...$$

```{r}
dlnorm(33, 3, 0.37)
```

А что если у нас не одно наблюдение, а несколько? Например, мы наблюдаем языки с 33 и 26 согласными? События независимы друг от друга, значит, мы можем перемножить получаемые вероятности. Так как результат перемножения маленьких чисел дает маленькие числа, с которыми компьютер не всегда хорошо справляется, для практических вычислений вместо функции правдоподобия используют ее натуральный логарифм (логарифмическая функция правдоподобия).

```{r, echo=FALSE}
tibble(x = 0:80) %>% 
  ggplot(aes(x)) +
  stat_function(fun = function(x) dlnorm(x, 3, 0.37))+
  geom_segment(aes(x = 33, xend = 33, y = 0, yend = dlnorm(33, 3, 0.37)), color = "red")+
  geom_segment(aes(x = 33, xend = 0, y = dlnorm(33, 3, 0.37), yend = dlnorm(33, 3, 0.37)), color = "red",
               arrow = arrow(length = unit(0.03, "npc")))+
  geom_segment(aes(x = 26, xend = 26, y = 0, yend = dlnorm(26, 3, 0.37)), color = "red")+
  geom_segment(aes(x = 26, xend = 0, y = dlnorm(26, 3, 0.37), yend = dlnorm(26, 3, 0.37)), color = "red",
               arrow = arrow(length = unit(0.03, "npc")))+
  scale_x_continuous(breaks = c(0:4*20, 33, 26))+
  scale_y_continuous(breaks = c(0:3*0.02, round(c(dlnorm(33, 3, 0.37), dlnorm(26, 3, 0.37)), 3)))+
  labs(title = "Количество согласных в языках мира",
       caption = "по данным PHOIBLE",
       x = "количество согласных",
       y = "")
```

```{r}
dlnorm(33, 3, 0.37)*dlnorm(26, 3, 0.37)
```

В итоге:

* вероятность --- P(data|distribution parameter(s))
* правдоподобие --- L(distribution parameter(s)|data)

Интеграл распределения или сумма вероятностей равен 1. [Интеграл функции правдоподобия может быть не равен 1](https://stats.stackexchange.com/a/31241/225843).

**The Law of Likelihood** states that "within the framework of a statistical model, a particular set of data supports one statistical hypothesis better than another if the likelihood of the first hypothesis, on the data, exceeds the likelihood of the second hypothesis” (Edwards, 1992, p. 30).

## 2. Отношение правдободобий

Важно понимать, что само по себе значение правдоподобия бессмысленно, оно важно для сравнения со значениями правдоподобия разных моделей. Представим, что мы пытаемся выбрать между двумя моделями:

* $H_1 = X \sim \ln\mathcal{N}(\mu = 3,\, \sigma^{2}= 0.37)$
* $H_2 = X \sim \ln\mathcal{N}(\mu = 3.5,\, \sigma^{2}= 0.25)$

```{r, echo= FALSE}
tibble(x = 0:80) %>% 
  ggplot(aes(x)) +
  geom_abline(intercept = 0, slope = 0, linetype = 2)+
  stat_function(fun = function(x) dlnorm(x, 3, 0.37))+
  stat_function(fun = function(x) -dlnorm(x, 3.5, 0.25))+
  geom_segment(aes(x = 33, xend = 33, y = 0, yend = dlnorm(33, 3, 0.37)), color = "red")+
    geom_segment(aes(x = 33, xend = 33, y = 0, yend = -dlnorm(33, 3.5, 0.25)), color = "darkgreen")+
  geom_segment(aes(x = 33, xend = 0, y = dlnorm(33, 3, 0.37), yend = dlnorm(33, 3, 0.37)), color = "red",
               arrow = arrow(length = unit(0.03, "npc")))+
    geom_segment(aes(x = 33, xend = 0, y = -dlnorm(33, 3.5, 0.25), yend = -dlnorm(33, 3.5, 0.25)), color = "darkgreen",
               arrow = arrow(length = unit(0.03, "npc")))+
  geom_segment(aes(x = 26, xend = 26, y = 0, yend = dlnorm(26, 3, 0.37)), color = "red")+
  geom_segment(aes(x = 26, xend = 26, y = 0, yend = -dlnorm(26, 3.5, 0.25)), color = "darkgreen")+
  geom_segment(aes(x = 26, xend = 0, y = dlnorm(26, 3, 0.37), yend = dlnorm(26, 3, 0.37)), color = "red",
               arrow = arrow(length = unit(0.03, "npc")))+
  geom_segment(aes(x = 26, xend = 0, y = -dlnorm(26, 3.5, 0.25), yend = -dlnorm(26, 3.5, 0.25)), color = "darkgreen",
               arrow = arrow(length = unit(0.03, "npc")))+
  scale_x_continuous(breaks = c(0:4*20, 33, 26))+
  scale_y_continuous(breaks = c(-2:2*0.05, round(c(dlnorm(33, 3, 0.37), dlnorm(26, 3, 0.37), -dlnorm(33, 3.5, 0.25), -dlnorm(26, 3.5, 0.25)), 3)), labels = abs(c(-2:2*0.05, round(c(dlnorm(33, 3, 0.37), dlnorm(26, 3, 0.37), -dlnorm(33, 3.5, 0.25), -dlnorm(26, 3.5, 0.25)), 3))))+
  annotate(geom = "text", x = 55, y = c(0.03, -0.03), label = c("lnN(μ = 3, σ²=0.37)", "lnN(μ = 3.5, σ²=0.25)"), size = 5)+
  labs(title = "Количество согласных в языках мира",
       caption = "по данным PHOIBLE (верхний)",
       x = "количество согласных",
       y = "")
```

```{r}
L1 <- dlnorm(33, 3, 0.37)*dlnorm(26, 3, 0.37)
L2 <- dlnorm(33, 3.5, 0.25)*dlnorm(26, 3.5, 0.25)
L2/L1
```

Как мы видим, на основании наших (фейковых) данных $H_2$ в 4 раза более вероятнее, чем $H_1$. Надо отметить, что не все тепло относятся к сравнению моделей (см. [Gelman, Rubin 1994](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.6443)).

## 3 Формула Байеса опять

Представим себе, что у нас есть $k$ гипотез $M$. Тогда формула Байеса может выглядеть вот так:

$$P(θ|Data, M_k) = \frac{P(Data|θ, M_k) \times  P(θ| M_k) }{P(Data|M_k)}$$

Коэффициент Байеса определяют как соотношение предельных правдоподобий ($P(Data, M_k)$) моделей (в принципе их может быть больше двух):

$$
BF_{12} = \frac{P(Data | M_1 )}{P(Data | M_2)}
$$

Вычислять предельные правдоподобия порой достаточно сложно, так что иногда используют численную аппроксимацию.

### 3.1 Биномиальный вариант

Рассмотрим пример эксперимента Бернулли:

* мы посчитали количество букв "а" в рассказе А. П. Чехова и получили 58 букв из рассказа длинной 699 букв (пробелы и латинские буквы выкинуты);
* представим, что у нас есть две модели, соогласно одной мы ожидаем долю 0.08, а согласно другой 0.085.

Мы помним, что эксперимент Бернулли описывается биномиальным распределением:

$$P(k | n, p) = \frac{n!}{k!(n-k)!} \times p^k \times (1-p)^{n-k} =  {n \choose k} \times p^k \times (1-p)^{n-k}$$ 

Так что в случае наших моделей будет:

$$P(Data | M_1) = {n \choose k} \times p^k \times (1-p)^{n-k} = {699 \choose 58} \times 0.08^{58} \times (1-0.08)^{699-58} = 0.0523985$$ 
```{r}
dbinom(58, 699, prob = 0.08)
```

$$P(Data | M_2) = {n \choose k} \times p^k \times (1-p)^{n-k} = {699 \choose 58} \times 0.085^{58} \times (1-0.085)^{699-58} = 0.04402509$$ 
```{r}
dbinom(58, 699, prob = 0.09)
```

Тогда коэфициент Байеса будет

```{r}
BF_12 = dbinom(58, 699, prob = 0.08)/dbinom(58, 699, prob = 0.09)
BF_12
```

```{r, echo=FALSE}
tibble(x = 0:699,
       m_1 = dbinom(x, size = 699, prob = 0.08),
       m_2 = -dbinom(x, size = 699, prob = 0.09)) %>%
  gather(model, value, m_1:m_2) %>% 
  mutate(model = ifelse(x == 58, "result", model)) %>% 
  filter(x < 100) %>% 
  ggplot(aes(x, value, fill = model))+
  geom_col()+
  scale_y_continuous(breaks = c(-0.03, 0, 0.03, 0.06), labels = c(0.03, 0, 0.03, 0.06))
```

### 3.2 [Интерпретация коэффициента Байеса](https://en.wikipedia.org/wiki/Bayes_factor#Interpretation)


### 3.3 Дискретный вариант

[В датасете c грибами](https://raw.githubusercontent.com/agricolamz/2019_BayesDan_winter/master/datasets/mushrooms.csv) (взят c [kaggle](https://www.kaggle.com/uciml/mushroom-classification)) представлено следующее распределение по месту обитания:

```{r, echo = FALSE, message=FALSE}
df <- read_csv("https://github.com/agricolamz/2019_BayesDan_winter/blob/master/datasets/mushrooms.csv?raw=true")
df %>% 
  count(class, habitat) %>% 
  group_by(class) %>% 
  mutate(prop = n/sum(n)) %>% 
  ggplot(aes(class, prop, fill = habitat, label = round(prop, 3)))+
  geom_col()+
  geom_text(position = position_stack(vjust = 0.5), color = "white")
```

Мы нашли некоторый новый вид грибов на лужайке (`grasses`), а потом в лесу (`woods`). Давайте посчитаем $BF_{edible\ poisonous}$:


$$L(grasses,\ wood|edible) = 0.335 \times 0.447 = 0.149745$$

$$L(grasses,\ wood|poisonous) = 0.189 \times 0.324 = 0.061236$$

$$BF_{edible\ poisonous} = \frac{L(grasses,\ wood|edible)}{L(grasses,\ wood|poisonous)} = \frac{0.149745}{0.061236} = 2.445375$$

### 3.4

Вашего друга похитили а на почту отправили [датасет](https://raw.githubusercontent.com/agricolamz/2019_BayesDan_winter/master/datasets/weather.csv), в котором записаны данные о погоде из пяти городов. Ваш телефон зазвонил, и друг сказал, что не знает куда его похитили, но за окном легкий дождь (`Rain`). А на следующий день --- сильный дождь (`Rain Thunderstorm`). Посчитайте $BH_{San\_Diego\ Auckland}$ с точностью до 1 знака после запятой.

```{r, include=FALSE}
df <- read.csv("https://raw.githubusercontent.com/agricolamz/2019_BayesDan_winter/master/datasets/weather.csv")
df %>%
  count(city, events) %>% 
  group_by(city) %>% 
  mutate(prop = n/sum(n)) %>% 
  ggplot(aes(city, prop, fill = events, label = round(prop, 3)))+
  geom_col()+
  geom_text(position = position_stack(vjust = 0.5), color = "white")

df %>%
  count(city, events) %>% 
  group_by(city) %>% 
  mutate(prop = n/sum(n)) %>% 
  select(-n) %>% 
  spread(events, prop, fill = 0) %>% 
  mutate(likelihood_r_rt = `Rain , Thunderstorm` * Rain) ->
  results

BF <- round(results$likelihood_r_rt[5]/results$likelihood_r_rt[1], 3)
```

```{r, results="asis", echo=FALSE}
library(checkdown)
autocheck_question(question_id = 5.1, answer = BF)
```


### 3.5 Несколько точечных моделей

До сих пор мы рассматривали одну точечную модель, сравнивая доли 0.08 и 0.085.

* Мы посчитали количество букв “а” в рассказе А. П. Чехова и получили 58 букв из рассказа длинной 699 букв (пробелы и латинские буквы выкинуты);
* представим, что у нас есть две модели, соогласно одной мы ожидаем долю 0.08, а вторая модель состоит из 7 равновероятных моделей: 0.60 0.65 0.70 0.75 0.80 0.85 0.90.

Функцию правдоподобия для первой модели мы уже считали:

```{r}
dbinom(58, 699, prob = 0.08)
```

Функция правдоподобия второй модели -- это среднее функций правдоподобия всех входящих моделей: 

```{r}
mean(dbinom(58, 699, prob = seq(0.08, 0.085, 0.001)))
```

Байес фактор:

```{r}
mean(dbinom(58, 699, prob = seq(0.08, 0.085, 0.001)))/dbinom(58, 699, prob = 0.08)
```

Как видим, наша распределенная модель немного предпочтительнее, чем точечная.
